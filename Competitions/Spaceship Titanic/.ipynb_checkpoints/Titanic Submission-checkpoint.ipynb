{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T01:03:03.036941Z",
     "iopub.status.busy": "2022-11-07T01:03:03.036427Z",
     "iopub.status.idle": "2022-11-07T01:03:03.122997Z",
     "shell.execute_reply": "2022-11-07T01:03:03.121046Z",
     "shell.execute_reply.started": "2022-11-07T01:03:03.036836Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_dtype_convertion(X, dtypes):\n",
    "    for dtype, features in dtypes.items() :\n",
    "        if dtype == 'int' :\n",
    "            for feature in features :\n",
    "                if feature in X.columns :\n",
    "                    X[feature] = X[feature].astype('int', errors='ignore')\n",
    "        elif dtype == 'float32' :\n",
    "            for feature in features :\n",
    "                if feature in X.columns :\n",
    "                    X[feature] = X[feature].astype('float32', errors='ignore')\n",
    "        elif dtype == 'bool' :\n",
    "            for feature in features :\n",
    "                if feature in X.columns :\n",
    "                    X[feature] = X[feature].astype(np.bool, errors='ignore')                    \n",
    "        elif dtype == 'object' :\n",
    "            for feature in features :\n",
    "                if feature in X.columns :\n",
    "                    X[feature] = X[feature].astype('object', errors='ignore')  \n",
    "    return X\n",
    "\n",
    "def do_skewed_target_normalization(y): \n",
    "    return np.log(y)\n",
    "\n",
    "def do_skewed_features_normalization(X_train, X_test):\n",
    "    from scipy.stats import skew, norm, probplot\n",
    "    # Fetch all numeric features\n",
    "    numeric_features = X_train.dtypes[(X_train.dtypes != object)].index\n",
    "    skewed_features = X_train[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "    high_skew = skewed_features[skewed_features > 0.5]\n",
    "    skew_index = high_skew.index\n",
    "    # Normalize skewed features using log_transformation\n",
    "    X_train_nskew = X_train.copy()\n",
    "    X_test_nskew = X_test.copy()\n",
    "    for i in skew_index:\n",
    "        X_train_nskew[i] = np.log1p(X_train_nskew[i], where = X_train_nskew[i] > 0)\n",
    "        X_test_nskew[i] = np.log1p(X_test_nskew[i], where = X_test_nskew[i] > 0)\n",
    "    return X_train_nskew, X_test_nskew\n",
    "\n",
    "def do_missing_values_imputation(X, na_strategy):\n",
    "    for strategy, features in na_strategy.items() :\n",
    "        if strategy == 'median' :\n",
    "            for feature in features :\n",
    "                if feature in X.columns :\n",
    "                    X[feature].fillna(X[feature].median(), inplace=True)\n",
    "        elif strategy == 'mean' :\n",
    "            for feature in features :\n",
    "                if feature in X.columns :\n",
    "                    X[feature].fillna(X[feature].mean(), inplace=True)\n",
    "        elif strategy == 'most_common' :\n",
    "            for feature in features :\n",
    "                if feature in X.columns :\n",
    "                    X[feature].fillna(X[feature].mode()[0], inplace=True)\n",
    "        elif strategy == 'zero' :\n",
    "            for feature in features :\n",
    "                if feature in X.columns :\n",
    "                    X[feature].fillna(0, inplace=True)   \n",
    "    return X\n",
    "\n",
    "def do_feature_ordinal_encoding(X, mapping) :\n",
    "    for feature_name, map_dict in mapping.items() :\n",
    "        if feature_name in X.columns :\n",
    "            X[feature_name] = X[feature_name].map(map_dict)\n",
    "    return X\n",
    "\n",
    "def do_feature_encoding(X_train, X_test) :\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    object_cols = X_train.select_dtypes(include=['object']).columns\n",
    "    # X_train fitting & transformation\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols].astype(str)))\n",
    "    OH_cols.index = X_train.index\n",
    "    OH_cols.columns = OH_encoder.get_feature_names(object_cols)\n",
    "    X_train = X_train.drop(object_cols, axis=1)\n",
    "    X_train = pd.concat([X_train, OH_cols], axis=1)\n",
    "    # X_test transformation\n",
    "    OH_cols = pd.DataFrame(OH_encoder.transform(X_test[object_cols].astype(str)))\n",
    "    OH_cols.index = X_test.index\n",
    "    OH_cols.columns = OH_encoder.get_feature_names(object_cols)\n",
    "    X_test = X_test.drop(object_cols, axis=1)\n",
    "    X_test = pd.concat([X_test, OH_cols], axis=1)\n",
    "    return X_train, X_test\n",
    "\n",
    "def do_feature_clustering(X_train, X_test, y_train, features) :\n",
    "\n",
    "    def get_cluster_dict(table, feature, target, n_clusters = 5):\n",
    "        from sklearn.cluster import KMeans\n",
    "        clustering = KMeans(n_clusters=n_clusters, random_state = 42)\n",
    "        feature_describe = table.groupby([feature])[target].describe().fillna(0)\n",
    "        clustering.fit(feature_describe)\n",
    "        cluster_table = pd.DataFrame(zip(list(feature_describe.index),\n",
    "                                         list(feature_describe.loc[:,'mean']),\n",
    "                                         list(clustering.labels_)),\n",
    "                                         columns = ['feature','mean_target_value', 'cluster'])\n",
    "        cluster_dict = {}\n",
    "        print('Clustering for '+feature+' :')\n",
    "        for i in range(len(cluster_table.groupby('cluster')['feature'].unique())):\n",
    "            print(str(i)+'-'+str(cluster_table.groupby('cluster')['feature'].unique()[i]))\n",
    "            for f in cluster_table.groupby('cluster')['feature'].unique()[i]:\n",
    "                cluster_dict[f] = i\n",
    "        print()\n",
    "        return cluster_dict \n",
    "    \n",
    "    df_train = pd.concat([y_train, X_train], axis=1)\n",
    "    for feature, n_clusters in features.items():\n",
    "        if feature in X_train.columns :\n",
    "            cluster_dict = get_cluster_dict(df_train, feature, y_train.name, n_clusters)\n",
    "            X_test[feature] = X_test[feature].map(cluster_dict)\n",
    "            X_train[feature] = X_train[feature].map(cluster_dict)\n",
    "    \n",
    "    return X_train, X_test\n",
    "    \n",
    "def do_feature_scaling(X_train, X_test) :\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    numeric_cols = X_train.select_dtypes(exclude=['object']).columns\n",
    "    for col in X_train[numeric_cols].columns:\n",
    "        X_train[col] = scaler.fit_transform(X_train[[col]])\n",
    "        X_test[col] = scaler.transform(X_test[[col]])\n",
    "    return X_train, X_test\n",
    "\n",
    "def do_categorical_dimension_reduction(X_cat, min_counts) :\n",
    "    X_cat_counts = X_cat.value_counts()\n",
    "    mask = X_cat.isin(X_cat_counts[X_cat_counts < min_counts].index)\n",
    "    X_cat[mask] = 'Other'\n",
    "    return X_cat\n",
    "\n",
    "def drop_outliers_values(X_train, y_train) :\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    iso_forest = IsolationForest(random_state=0)\n",
    "    df_train = pd.concat([y_train, X_train], axis=1)\n",
    "    numeric_features = df_train.dtypes[(df_train.dtypes != object)].index\n",
    "    df_train_without_outlier = pd.Series(iso_forest.fit_predict(df_train[numeric_features].fillna(0)), index=df_train.index)\n",
    "    df_train = df_train.loc[df_train_without_outlier.index[df_train_without_outlier == 1], :]\n",
    "    X_train = df_train.drop(columns=y_train.name)\n",
    "    y_train = df_train.loc[:, y_train.name]\n",
    "    return X_train, y_train\n",
    "\n",
    "def adapt_test_set_to_train_set_data_structure(X_train, X_test) :\n",
    "    # Suppression des colonnes du test set non présentes dans le train set\n",
    "    X_test_corrected = X_test.drop(columns=list(set(X_test.columns.tolist())-set(X_train.columns.tolist())))\n",
    "    # Ajout des colonnes du train set non présentes dans le test set\n",
    "    X_test_corrected = pd.DataFrame(X_test_corrected, columns=X_train.columns.tolist())\n",
    "    # On applique la valeur 0 à ces nouvelles colonnes\n",
    "    X_test_corrected[list(set(X_train.columns.tolist())-set(X_test_corrected.columns.tolist()))] = 0\n",
    "    return X_test_corrected\n",
    "\n",
    "def build_classifier(classifier, \n",
    "                    init_hyperparameters, \n",
    "                    tuning_hyperparameters, \n",
    "                    hyperparameters, \n",
    "                    X_train, \n",
    "                    y_train, \n",
    "                    settings) :\n",
    "    \n",
    "    from datetime import datetime\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    from sklearn.metrics import roc_curve, roc_auc_score\n",
    "    from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "\n",
    "    def timer(start_time=None):\n",
    "        if not start_time:\n",
    "            start_time = datetime.now()\n",
    "            return start_time\n",
    "        elif start_time:\n",
    "            thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "            tmin, tsec = divmod(temp_sec, 60)\n",
    "            print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "    if settings['do_tuning'] not in [True, False] :\n",
    "        settings['do_tuning'] = False\n",
    "    if settings['do_kfold'] not in [True, False] :\n",
    "        settings['do_kfold'] = True\n",
    "    if settings['tuning_type'] not in ['grid', 'randomized', 'bayes'] :\n",
    "        settings['tuning_type'] = 'randomized'\n",
    "    if settings['tuning_n_iter'] > 1000 :\n",
    "        settings['tuning_n_iter'] = 1000        \n",
    "\n",
    "    if settings['do_tuning'] == True :\n",
    "        \n",
    "        estimator = classifier(**init_hyperparameters)\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        if settings['tuning_type'] == 'bayes' :\n",
    "            model = BayesSearchCV(estimator=estimator, \n",
    "                                    search_spaces = tuning_hyperparameters, \n",
    "                                    n_iter=settings['tuning_n_iter'], \n",
    "                                    scoring='roc_auc', \n",
    "                                    verbose=0,\n",
    "                                    cv=kf.split(X_train, y_train.values))\n",
    "\n",
    "        elif settings['tuning_type'] == 'randomized' :\n",
    "            model = RandomizedSearchCV(estimator=estimator, \n",
    "                                     param_distributions=tuning_hyperparameters, \n",
    "                                     n_iter=settings['tuning_n_iter'], \n",
    "                                     scoring='roc_auc', \n",
    "                                     verbose=0,\n",
    "                                     cv=kf.split(X_train, y_train.values))      \n",
    "\n",
    "        elif tuning_type == 'grid' :\n",
    "            model = GridSearchCV(estimator=estimator, \n",
    "                                   param_grid=tuning_hyperparameters,\n",
    "                                   scoring='roc_auc',\n",
    "                                   verbose=0,\n",
    "                                   cv=kf.split(X_train, y_train.values)) \n",
    "\n",
    "        start_time = timer(None)\n",
    "        model.fit(X_train, np.ravel(y_train))\n",
    "        timer(start_time)\n",
    "\n",
    "        print('Best accuracy for a single model : {}.'.format(model.best_score_))\n",
    "        print('Best hyperparameters: {}.'.format(model.best_params_))          \n",
    "        return model\n",
    "\n",
    "    else :\n",
    "        \n",
    "        start_time = timer(None)\n",
    "        estimator = classifier(**hyperparameters)\n",
    "\n",
    "        if settings['do_kfold'] == True:\n",
    "            kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            splits = kf.split(X_train, y_train.values)\n",
    "\n",
    "            r2 = []\n",
    "            auc_train = []\n",
    "            auc_val = []\n",
    "            for train_index, val_index in splits:\n",
    "                X_train_split, y_train_split = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "                X_val_split, y_val_split = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "                estimator.fit(X_train_split, np.ravel(y_train_split))\n",
    "                y_pred_val_split = estimator.predict(X_val_split)\n",
    "                y_pred_train_split = estimator.predict(X_train_split)\n",
    "                y_score_val_split = estimator.predict_proba(X_val_split)[:,1]\n",
    "                y_score_train_split = estimator.predict_proba(X_train_split)[:,1]\n",
    "                r2.append(estimator.score(X_val_split, y_val_split))\n",
    "                auc_val.append(roc_auc_score(y_val_split, y_score_val_split))\n",
    "                auc_train.append(roc_auc_score(y_train_split, y_score_train_split))\n",
    "\n",
    "            print('AUC - validation :', np.round(np.mean(auc_val), 3))\n",
    "            print('AUC - train :', np.round(np.mean(auc_train), 3))\n",
    "            print('Accuracy score :', np.round(np.mean(r2), 3))\n",
    "\n",
    "        estimator.fit(X_train, y_train.values)\n",
    "        timer(start_time)  \n",
    "        return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T01:03:03.126535Z",
     "iopub.status.busy": "2022-11-07T01:03:03.126061Z",
     "iopub.status.idle": "2022-11-07T01:03:05.369056Z",
     "shell.execute_reply": "2022-11-07T01:03:05.365161Z",
     "shell.execute_reply.started": "2022-11-07T01:03:03.126486Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# Loading dataset\n",
    "df_train = pd.read_csv('../input/spaceship-titanic/train.csv')\n",
    "X_test = pd.read_csv('../input/spaceship-titanic/test.csv')\n",
    "X_train = df_train.drop(columns=['Transported'])\n",
    "y_train = df_train['Transported']\n",
    "\n",
    "# Type \n",
    "dtypes = {\n",
    "    'int' : ['CryoSleep', 'VIP'],\n",
    "    'float32' : ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'],\n",
    "    'bool' : ['Transported'],\n",
    "    'object' : ['HomePlanet', 'Destination', 'Cabin', 'Name']\n",
    "}\n",
    "X_train = do_dtype_convertion(X_train, dtypes)\n",
    "X_test = do_dtype_convertion(X_test, dtypes)\n",
    "\n",
    "# Missing values imputation\n",
    "na_strategy = {\n",
    "    'median' : ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'],\n",
    "    'most_common' : ['HomePlanet', 'Destination', 'Cabin', 'Name', 'CryoSleep', 'VIP'],\n",
    "    'zero' : X_train.drop(columns=['CryoSleep', 'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', \n",
    "                                   'Spa', 'VRDeck', 'HomePlanet', 'Destination', 'Cabin', 'Name', 'VIP']).columns\n",
    "}\n",
    "X_train = do_missing_values_imputation(X_train, na_strategy)\n",
    "X_test = do_missing_values_imputation(X_test, na_strategy)\n",
    "\n",
    "# Feature creation\n",
    "def do_feature_creation(table):\n",
    "    \n",
    "    table['CryoSleep'] = (table['CryoSleep'] == True).astype('int')\n",
    "    table['VIP'] = (table['VIP'] == True).astype('int')\n",
    "    \n",
    "    table['PassengerGroupID'] = table['PassengerId'].fillna(0).astype(str).apply(lambda x: x.split('_')[0]).fillna(0).astype('int')\n",
    "    table['PassengerNumberID'] = table['PassengerId'].fillna(0).astype(str).apply(lambda x: x.split('_')[1]).fillna(0).astype('int')\n",
    "    occurence_passengerGroupId = table['PassengerId'].fillna(0).astype(str).apply(lambda x: x.split('_')[0]).fillna(0).value_counts()\n",
    "    table['GroupSize'] = table['PassengerId'].fillna(0).astype(str).apply(lambda x: occurence_passengerGroupId[x.split('_')[0]] if (x.split('_')[0] in occurence_passengerGroupId.keys()) & (x.split('_')[0] != 0) else 0)\n",
    "\n",
    "    table['CabinDeck'] = table['Cabin'].fillna(0).astype(str).apply(lambda x: x.split('/')[0]).fillna(0)\n",
    "    table['CabinNum'] = table['Cabin'].fillna(0).astype(str).apply(lambda x: x.split('/')[1]).astype('int').fillna(0)\n",
    "    table['CabinSide'] = table['Cabin'].fillna(0).astype(str).apply(lambda x: x.split('/')[2]).fillna(0)\n",
    "\n",
    "    occurence_cabins = table['Cabin'].fillna(0).astype(str).value_counts()\n",
    "    occurence_deck = table['Cabin'].fillna(0).astype(str).apply(lambda x: x.split('/')[0]).fillna(0).value_counts()\n",
    "    occurence_num = table['Cabin'].fillna(0).astype(str).apply(lambda x: x.split('/')[1]).fillna(0).value_counts()\n",
    "    table['CabinSize'] = table['Cabin'].fillna(0).astype(str).apply(lambda x: occurence_cabins[x] if (x in occurence_cabins.keys()) & (x != 0) else 0)\n",
    "    table['CabinDeckSize'] = table['Cabin'].fillna(0).astype(str).apply(lambda x: occurence_deck[x.split('/')[0]] if (x.split('/')[0] in occurence_deck.keys()) & (x.split('/')[0] != '0') else 0)\n",
    "    table['CabinNumSize'] = table['Cabin'].fillna(0).astype(str).apply(lambda x: occurence_num[x.split('/')[1]] if (x.split('/')[1] in occurence_num.keys()) & (x.split('/')[0] != '0') else 0)\n",
    "\n",
    "    table['TotalSpent'] = table['RoomService']+table['FoodCourt']+table['ShoppingMall']+table['Spa']+table['VRDeck']\n",
    "    \n",
    "    # Removing non-alphanumeric characters\n",
    "    table['Destination'] = table['Destination'].str.replace('[^a-zA-Z0-9]', '', regex=True).str.strip()\n",
    "    \n",
    "    return table\n",
    "\n",
    "X_train = do_feature_creation(X_train)\n",
    "X_test = do_feature_creation(X_test)\n",
    "\n",
    "# Feature selection\n",
    "columns_to_exclude = ['PassengerId', 'Cabin', 'Name', 'CabinDeckSize']\n",
    "X_train = X_train.drop(columns_to_exclude, axis=1)\n",
    "X_test = X_test.drop(columns_to_exclude, axis=1)\n",
    "\n",
    "# Ordinal features encoding\n",
    "mapping = {\n",
    "    'CabinDeck' : {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7},\n",
    "    'CabinSide' : {'T':1, 'P':2, 'S':3}\n",
    "}\n",
    "X_train = do_feature_ordinal_encoding(X_train, mapping)\n",
    "X_test = do_feature_ordinal_encoding(X_test, mapping)\n",
    "\n",
    "# Feature encoding\n",
    "X_train, X_test = do_feature_encoding(X_train, X_test)\n",
    "\n",
    "# Dummy encoding\n",
    "X_train = pd.get_dummies(X_train, prefix_sep='_', drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, prefix_sep='_', drop_first=True)\n",
    "\n",
    "# Features synchronization\n",
    "X_test = adapt_test_set_to_train_set_data_structure(X_train, X_test)\n",
    "\n",
    "X_train = X_train.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T01:03:05.372019Z",
     "iopub.status.busy": "2022-11-07T01:03:05.371508Z",
     "iopub.status.idle": "2022-11-07T01:03:16.904846Z",
     "shell.execute_reply": "2022-11-07T01:03:16.903808Z",
     "shell.execute_reply.started": "2022-11-07T01:03:05.371976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - validation : 0.902\n",
      "AUC - train : 0.936\n",
      "Accuracy score : 0.803\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 11.29 seconds.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_xgb = build_classifier(classifier = XGBClassifier, \n",
    "                            init_hyperparameters = {\n",
    "                                'n_estimators': 100, \n",
    "                                'random_state': 42, \n",
    "                                'use_label_encoder': False\n",
    "                            },\n",
    "                            tuning_hyperparameters = {\n",
    "                                'scale_pos_weight' : [1, 1.3, 1.6],\n",
    "                                'objective'  : ['reg:logistic'],\n",
    "                                'alpha' : [0.0001, 0.01, 0.1, 1, 5, 10],\n",
    "                                'gamma' : [1, 2, 5, 8, 10, 14, 18, 30],\n",
    "                                'reg_lambda' : [1, 2, 5, 8, 10, 14, 18, 30],\n",
    "                                'learning_rate' : [0.001, 0.01, 0.1, 1, 5, 10],\n",
    "                                'colsample_bytree' : [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "                                'max_delta_step': [2, 4, 6, 8, 10, 15, 20, 50],\n",
    "                                'min_child_weight' : [5, 10, 15, 20, 25, 30, 50],\n",
    "                                'subsample' : [0.60, 0.70, 0.80, 0.85, 0.90, 0.95],\n",
    "                                'max_depth': [3, 6, 9, 15, 20, 25, 30, 35, 45, None]\n",
    "                            }, \n",
    "                            hyperparameters = {\n",
    "                                 'subsample': 0.85, \n",
    "                                 'scale_pos_weight': 1.6, \n",
    "                                 'reg_lambda': 1, \n",
    "                                 'objective': 'reg:logistic', \n",
    "                                 'min_child_weight': 10, \n",
    "                                 'max_depth': 45, \n",
    "                                 'max_delta_step': 20, \n",
    "                                 'learning_rate': 0.1, \n",
    "                                 'gamma': 8, \n",
    "                                 'colsample_bytree': 0.8, \n",
    "                                 'alpha': 5,\n",
    "                                 'use_label_encoder': False\n",
    "                            },  \n",
    "                            X_train = X_train, \n",
    "                            y_train = y_train, \n",
    "                            settings = {\n",
    "                                'do_tuning' : False,\n",
    "                                'do_kfold' : True,\n",
    "                                'tuning_type' : 'randomized',\n",
    "                                'tuning_n_iter' : 60\n",
    "                            }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T01:03:16.911247Z",
     "iopub.status.busy": "2022-11-07T01:03:16.910198Z",
     "iopub.status.idle": "2022-11-07T01:03:24.946580Z",
     "shell.execute_reply": "2022-11-07T01:03:24.945643Z",
     "shell.execute_reply.started": "2022-11-07T01:03:16.911198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - validation : 0.88\n",
      "AUC - train : 0.898\n",
      "Accuracy score : 0.794\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 7.81 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model_adb = build_classifier(classifier = AdaBoostClassifier,\n",
    "                            init_hyperparameters = {\n",
    "                                'base_estimator' : SVC(probability=True, kernel='linear')\n",
    "                            },\n",
    "                            tuning_hyperparameters = {\n",
    "                                'n_estimators' : [20, 50, 100, 200],\n",
    "                                'learning_rate': [0.01, 0.05, 0.10, 1],\n",
    "                                'algorithm' : ['SAMME', 'SAMME.R']\n",
    "                            }, \n",
    "                            hyperparameters = {\n",
    "                                'n_estimators': 100,\n",
    "                                'learning_rate': 1, \n",
    "                            }, \n",
    "                            X_train = X_train, \n",
    "                            y_train = y_train, \n",
    "                            settings = {\n",
    "                                'do_tuning' : False,\n",
    "                                'do_kfold' : True,\n",
    "                                'tuning_type' : 'randomized',\n",
    "                                'tuning_n_iter' : 20\n",
    "                            }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T01:03:24.948376Z",
     "iopub.status.busy": "2022-11-07T01:03:24.947865Z",
     "iopub.status.idle": "2022-11-07T01:03:37.282727Z",
     "shell.execute_reply": "2022-11-07T01:03:37.281533Z",
     "shell.execute_reply.started": "2022-11-07T01:03:24.948344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - validation : 0.894\n",
      "AUC - train : 0.995\n",
      "Accuracy score : 0.805\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 12.33 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = build_classifier(classifier = RandomForestClassifier, \n",
    "                            init_hyperparameters = {\n",
    "                                'n_estimators' : 300, \n",
    "                                'random_state' : 42\n",
    "                            },\n",
    "                            tuning_hyperparameters = {\n",
    "                                'bootstrap': [True, False],\n",
    "                                'criterion' : ['gini', 'entropy'],\n",
    "                                'max_features': ['auto', 'sqrt'],\n",
    "                                'max_depth': [10, 20, 30, 40, 50, 60, 70, None],\n",
    "                                'min_samples_leaf': [1, 2, 4, 8, 20],\n",
    "                                'min_samples_split': [2, 5, 10]\n",
    "                            }, \n",
    "                            hyperparameters = {\n",
    "                                'min_samples_split': 10, \n",
    "                                'min_samples_leaf': 4, \n",
    "                                'max_features': 'auto', \n",
    "                                'max_depth': None, \n",
    "                                'criterion': 'entropy', \n",
    "                                'bootstrap': False\n",
    "                            },\n",
    "                            X_train = X_train, \n",
    "                            y_train = y_train, \n",
    "                            settings = {\n",
    "                                'do_tuning' : False,\n",
    "                                'do_kfold' : True,\n",
    "                                'tuning_type' : 'randomized',\n",
    "                                'tuning_n_iter' : 60\n",
    "                            })  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T01:03:37.284630Z",
     "iopub.status.busy": "2022-11-07T01:03:37.284159Z",
     "iopub.status.idle": "2022-11-07T01:03:37.528777Z",
     "shell.execute_reply": "2022-11-07T01:03:37.527505Z",
     "shell.execute_reply.started": "2022-11-07T01:03:37.284590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - validation : 0.866\n",
      "AUC - train : 0.907\n",
      "Accuracy score : 0.775\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 0.23 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_dt = build_classifier(classifier = DecisionTreeClassifier, \n",
    "                            init_hyperparameters = {},\n",
    "                            tuning_hyperparameters = {\n",
    "                                'max_depth' : [2, 4, 5, 6, 7, 8, 10, 15, 20],\n",
    "                                'min_samples_split' : [2, 4, 6, 8, 10, 20, 30],\n",
    "                                'min_samples_leaf' : [2, 4, 6, 8, 10, 20, 30],\n",
    "                                'max_features' : [4, 6, 8, 10, 15, 20],\n",
    "                                'splitter' : ['best', 'random']\n",
    "                            }, \n",
    "                            hyperparameters = {\n",
    "                                'splitter': 'best', \n",
    "                                'min_samples_split': 20, \n",
    "                                'min_samples_leaf': 30, \n",
    "                                'max_features': 10, \n",
    "                                'max_depth': 15\n",
    "                            }, \n",
    "                            X_train = X_train, \n",
    "                            y_train = y_train, \n",
    "                            settings = {\n",
    "                                'do_tuning' : False,\n",
    "                                'do_kfold' : True,\n",
    "                                'tuning_type' : 'randomized',\n",
    "                                'tuning_n_iter' : 20\n",
    "                            }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T01:03:37.531003Z",
     "iopub.status.busy": "2022-11-07T01:03:37.530655Z",
     "iopub.status.idle": "2022-11-07T01:03:53.268922Z",
     "shell.execute_reply": "2022-11-07T01:03:53.267885Z",
     "shell.execute_reply.started": "2022-11-07T01:03:37.530973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - validation : 0.902\n",
      "AUC - train : 0.94\n",
      "Accuracy score : 0.808\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 14.76 seconds.\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model_lgbm = build_classifier(classifier = LGBMClassifier, \n",
    "                            init_hyperparameters = {\n",
    "                                'objective' : 'binary', \n",
    "                                'random_state' : 1\n",
    "                            },\n",
    "                            tuning_hyperparameters = {\n",
    "                                'objective' : ['binary'],\n",
    "                                'boosting_type': ['dart'],\n",
    "                                'num_leaves': np.arange(10, 15, 1),\n",
    "                                'max_depth': np.arange(6, 8, 1),\n",
    "                                'learning_rate': np.arange(0.07, 0.12, 0.01),\n",
    "                                'n_estimators': np.arange(480, 510, 2),\n",
    "                                'reg_alpha': np.arange(0.4, 0.6, 0.02), \n",
    "                                'min_child_samples': np.arange(40, 60, 2),\n",
    "                                'reg_lambda': np.arange(0.85, 1, 0.01)\n",
    "\n",
    "                            }, \n",
    "                            hyperparameters = {\n",
    "                                'reg_lambda': 1.0,\n",
    "                                'reg_alpha': 0.48,\n",
    "                                'num_leaves': 10,\n",
    "                                'n_estimators': 508,\n",
    "                                'min_child_samples': 52,\n",
    "                                'max_depth': 6,\n",
    "                                'learning_rate': 0.10,\n",
    "                                'boosting_type': 'dart'\n",
    "                            },\n",
    "                            X_train = X_train, \n",
    "                            y_train = y_train, \n",
    "                            settings = {\n",
    "                                'do_tuning' : False,\n",
    "                                'do_kfold' : True,\n",
    "                                'tuning_type' : 'randomized',\n",
    "                                'tuning_n_iter' : 40\n",
    "                            }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T01:03:53.275742Z",
     "iopub.status.busy": "2022-11-07T01:03:53.273150Z",
     "iopub.status.idle": "2022-11-07T01:06:55.351034Z",
     "shell.execute_reply": "2022-11-07T01:06:55.349669Z",
     "shell.execute_reply.started": "2022-11-07T01:03:53.275697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - validation : 0.881\n",
      "AUC - train : 0.993\n",
      "Accuracy score : 0.795\n",
      "\n",
      " Time taken: 0 hours 3 minutes and 2.06 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "model_stacking = build_classifier(classifier = StackingClassifier, \n",
    "                                init_hyperparameters = {},\n",
    "                                tuning_hyperparameters = {}, \n",
    "                                hyperparameters = {\n",
    "                                    'estimators' : [\n",
    "                                        ('xgb', model_xgb),\n",
    "                                        ('rf', model_rf),\n",
    "                                        ('lgbm', model_lgbm)\n",
    "                                    ], \n",
    "                                    'final_estimator' : RandomForestClassifier(**{\n",
    "                                        'n_estimators' : 50,\n",
    "                                        'min_samples_split': 2, \n",
    "                                        'min_samples_leaf': 2, \n",
    "                                        'max_features': 'sqrt', \n",
    "                                        'max_depth': 5, \n",
    "                                        'criterion': 'gini', \n",
    "                                        'bootstrap': False\n",
    "                                    }),\n",
    "                                    'cv' : 5\n",
    "                                },\n",
    "                                X_train = X_train, \n",
    "                                y_train = y_train, \n",
    "                                settings = {\n",
    "                                    'do_tuning' : False,\n",
    "                                    'do_kfold' : True,\n",
    "                                    'tuning_type' : 'randomized',\n",
    "                                    'tuning_n_iter' : 30\n",
    "                                }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T01:07:09.121192Z",
     "iopub.status.busy": "2022-11-07T01:07:09.120652Z",
     "iopub.status.idle": "2022-11-07T01:07:09.225938Z",
     "shell.execute_reply": "2022-11-07T01:07:09.224919Z",
     "shell.execute_reply.started": "2022-11-07T01:07:09.121146Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = model_lgbm.predict(X_test)\n",
    "submission = pd.read_csv('../input/spaceship-titanic/sample_submission.csv')\n",
    "submission['Transported'] = prediction\n",
    "submission.to_csv('./submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
